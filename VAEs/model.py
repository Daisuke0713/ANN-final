'''
    Convolutional VAEs based on the following references:
    https://www.tensorflow.org/tutorials/generative/cvae
    https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73
    https://arxiv.org/pdf/1907.08956.pdf

    Slight modifications were made to further improve the model
'''

import numpy as np
import time
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras, random, nn
from keras import Model, Sequential, optimizers, metrics
from keras.layers import InputLayer, Dense, Conv2D, Conv2DTranspose, Reshape, Flatten

'''Basic Convolutional VAE'''
class VAE(Model):

    def __init__(self, latent_dim):
        super(VAE, self).__init__()
        self.latent_dim = latent_dim
        self.encoder = self.generate_encoder()
        self.decoder = self.generate_decoder()

    def generate_encoder(self):
        return Sequential([
            InputLayer(input_shape=(28, 28, 1)),
            Conv2D(filters=32, kernel_size=3, strides=(2,2), activation='relu'),
            Conv2D(filters=64, kernel_size=3, strides=(2,2), activation='relu'),
            Flatten(),
            # because the output of encoder has two parts, the mean and 
            # log-variance of the posterior distribution q(z|x)
            Dense(self.latent_dim * 2)
        ])
    
    def generate_decoder(self):
        return Sequential([
            # the input to decoder is z sampled from the Gaussian (through 
            # reparameterization trick)
            InputLayer(input_shape=(self.latent_dim,)),
            Dense(units=32*7*7, activation='relu'),
            Reshape(target_shape=(7,7,32)),
            Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu'),
            Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu'),
            Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same')
        ])

    def encode(self, x):
        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
        return mean, logvar
    
    def reparameterize(self, mean, logvar):
        e = random.normal(shape=mean.shape)
        z = mean + e * tf.exp(logvar * 0.5)
        return z

    def decode(self, z):
        xhat = self.decoder(z)
        return xhat
    
    '''used during inference'''
    def sample(self, rand_vec=None):
        if rand_vec == None:
            rand_vec = random.normal(shape=(100, self.latent_dim))
        return tf.sigmoid(self.decode(rand_vec)) # binarize the result for visibility


'''Define loss function and training proceedures'''

def log_normal(sample, mean, logvar):
    prob = -0.5 * ((sample - mean) ** 2.0 * tf.exp(-logvar) + logvar + tf.math.log(2.0 * np.pi))
    return tf.reduce_sum(prob, axis=1)

def get_loss(model, x):
    mean, logvar = model.encode(x) # defining q(z|x)
    z = model.reparameterize(mean, logvar) # sampled from q(z|x)
    xhat = model.decode(z) # generated by obtaining p(x|z)
    prob = nn.sigmoid_cross_entropy_with_logits(labels=x, logits=xhat)

    logpxz = -tf.reduce_sum(prob, axis=[1,2,3]) # just reconstruction error
    logpz = log_normal(z, mean, logvar) # standard Gaussian (assumed)
    logqzx = log_normal(z, mean, logvar) # Gaussian obtained through encoder

    # return the average value for each sample within this batch
    return -tf.reduce_mean(logpxz + logpz - logqzx)

'''use Adan'''
def train_per_batch(model, x):
    optimizer = optimizers.Adam(learning_rate=1e-4)
    with tf.GradientTape() as gt:
        loss = get_loss(model, x)
        gradient = gt.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradient, model.trainable_variables))
    return loss

def train(model, device_name, epochs, x, x_test=None):
    # for each epoch
    with tf.device(device_name=device_name):
        for epoch in range(epochs):
            start_time = time.time()
            train_loss = metrics.Mean()
            for x_batch in x:
                train_loss(train_per_batch(model, x_batch))
            time_elapsed = time.time() - start_time
            loss = train_loss.result()
            print(f'Epoch: {epoch}, train loss: {loss}, time elapsed: {time_elapsed}')

            if x_test is not None:
                for test_batch in x_test:
                    x_sample = test_batch[0:16,:,:,:]
                    break
                pred = predict(model=model, x=x_sample)
                show_pred(pred, epoch)

def predict(model, x):
    mean, logvar = model.encode(x) 
    z = model.reparameterize(mean, logvar) 
    return model.sample(z) 

def show_pred(pred, epoch=None):
    plt.figure(figsize=(4,4))
    for i in range(16):
        plt.subplot(4, 4, i + 1)
        plt.imshow(pred[i, :, :, 0], cmap='gray')
        plt.axis('off')
    if epoch is not None:
        plt.savefig(f'./digits_images/epoch{epoch}.png')
    else:
        plt.show()